<nav class="well">
  <ul class="nav nav-list">
    <li class="nav-header">Overview</li>
    <li><a data-nonpushstate href="#intro">Introduction</a></li>
    <li><a data-nonpushstate href="#type">Type of cloud service</a></li>
    <li><a data-nonpushstate href="#what">Languages / protocols</a></li>
    <li class="nav-header">Continuity of service</li>
    <li><a data-nonpushstate href="#backups">Backups / disaster recovery</a></li>
    <li><a data-nonpushstate href="#leaving">Should you wish to leave</a></li>
  </ul>
</nav>

<div class="wrapper">

<h2 id="intro">ScraperWiki for Corporates</h2>

<p>ScraperWiki helps people manage datasets, and the tools needed to get, clean
and analyse data.</p>

<p>This page describes the <b>corporate</b> version of ScraperWiki, covering
matters relevant to legal, IT and financial staff. Levels of service for
non-corporate level customers are lower, and are described in separate terms
and conditions. End user documentation is also separate.</p>

<p>We undertake to describe all aspects of our service clearly and
transparently. If you have any questions, please get in touch and help us 
improve this living document.</p>

<h2 id="type">Type of cloud service</h2>

<p>ScraperWiki is a hybrid of SaaS and PaaS. It provides end user tools like a
SaaS, yet allows visibility and customisation of all source code like a PaaS.
This is because data tools need to be <b>easy</b> for anyone to use in most cases, yet
<b>powerful</b> enough for those who can program to handle edge cases when needed.
</p>

<p>ScraperWiki is deployed in a Public Cloud model. It is hosted on IaaS
servers at cloud provider <a href="http://www.linode.com/">Linode</a>, at its
London datacentre. We meet the tiering standards that Linode meet.
</p>

<p>We currently scale by provisioning for the entire platform as required, as
we are a platform for collaboration on (relatively) small amounts of data. We
outsource any large data volumes to other providers. By default we offer
non-guaranteed performance, we can offer guaranteed for a larger cost.
Utilisation reporting, where there are restrictions, is built into the platform
and its API.</p>

<p>Our platform can be freely used by our customers in any way they like. In
particular, customers can contract third parties to adapt, extend or write new
data tools and applications which run on the platform, as they want.
</p>

<h2 id="what">Languages / protocols</h2>

<p>ScraperWiki is a web platform for data science. Subscribers to ScraperWiki can
run scripts which get, clean and analyse data &ndash; such as scrapers and
visualisations. It can be used to gather information from disparate sources,
including websites, FTP servers, legacy database systems, PDFs, and then export
that in a common format (Excel, JSON, CSV), or as interactive web
visualisations.
</p>

<p>ScraperWiki supports any open source programming language &ndash; Python, R, Perl,
Java and so on.  Schema-light SQL datasets (based on SQLite or PostgreSQL).
Integration between datasets and views using web-standards based SQL API, which
returns JSON over a REST interface. Full POSIX filesystem.  Integrates with SSH, SFTP, SCP, Git, Subversion and
other standard Unix tools.  Data export over HTTP, data visualisation using
HTML/Javascript working with any of the backend languages.
</p>

<p class="well well-small"><span class="label label-info">Top tip!</span> 
You can run any other Unix software where needed, including binary only
software (e.g. Abbyy's PDF reader) &ndash; each dataset is a full virtual
Linux shell account.
</p> 


<h2 id="backups">Backups / disaster recovery</h2>

<p>All dataset files and data are stored on a distributed GlusterFS filesystem,
mirrored with two copies. Metadata is stored in a MongoDB cluster on MongoHQ’s
cloud service.
</p>

<p>Our backup strategy is based on two tiers of backups, one on Linode, and the
other redundantly on Amazon S3. These are five sequentially worse disaster
situations, with recovery times indicated.</p>

<ol>
<li>Individual machines are designed to be ephemeral, and provision is fully
automated. We can rebuild individual machines in about <b>20 minutes</b>, and the
whole cluster in under <b>2 hours</b>. MongoDB is backed up to Amazon S3, where we can
restore in under <b>1 hour</b>.
</li>
<li>Should multiple GlusterFS servers fail, or a software bug damage the
GlusterFS servers, we have a Linode whole machine image backup of them. This is
a daily rotating backup, which can be recovered within about <b>an hour</b>.
</li>
<li>We make daily rotating differential encrypted backups of the GlusterFS
filesystem using Duplicity, with weekly full backups. These are stored in
Amazon S3. We can restore the whole filesystem in under <b>1 day</b>, and corporate
plan customers in under <b>2 hours</b>.
</li>
<li>Should Linode’s London data centre fail, they have additional data centres
in the US and Japan which we can restore to in under <b>1 day</b> for all user, under
2 hours for corporate customers.
</li>
<li>
If the whole of Linode fails, then we will recover to a new cluster on Amazon
EC2. This will currently require updating of some systems administration
scripts, so will take <b>2 days</b>.
</li>
</ol>

<h2 id="leaving">Should you wish to leave</h2>

<p>We try to keep your custom by offering excellent service, not by locking you
in.</p>

<p>Your data and files are available programmatically at all times, via our
APIs and using the SFTP file transfer protocol. You can use this to make your
own backups, or to migrate data off ScraperWiki. There is no extra charge for this.
Scrapers and views are written using standard open source tools and
protocols, making it relatively easy to migrate individual applications to your
own Linux servers.</p>

<p>In addition, the ScraperWiki platform itself and the core tools are all open
source (see our <a href="http://github.com/scraperwiki">GitHub account</a>).
This gives you the ultimate protection of being able to host it yourself, or 
pay another organisation to host it for you.
</p>

<p class="well well-small"><span class="label label-important">Purge data!</span> 
Should it be required for compliance or other reasons, data can be 
destroyed. This can be done at any time by administrators of the account, or by
asking us to. It will remain in backups until they are rotated out.
</p>

<!-- TODO: -->

<!-- For current status, and uptime history, see our server monitoring at 
<a href="http://status.scraperwiki.com/">status.scraperwiki.com</a>. -->

<!-- Ordering, invoicing and on boarding - how to place order, and set up admin
users and so on -->

<!-- Information assurance - legal requirements -->

<!-- Termination - how to terminate, typically year notice -->

<!-- Pricing, including trial -->

<!-- Service constraints - Maintenance window, customisability, coping with deprecation -->
<!-- Our platform is completely customisable. End users can see the source code
of the data tools, and alter, fork and improve them. You can do what you like.

Old tools are not directly deprecated, they will continue to work as each place
they are used there is a fork of the code. Platform facilities will generally
be altered in such a way as to not alter the function of existing tools. Where
platform features need to be deprecated, end users will be given six months
notice, and help migrating any old tools they have which rely on the changing
features.  -->

<!-- Documentation, tutorials and training 
ScraperWiki is open source, and has a thriving community. This supplements paid
for support. Questions can be asked and answered on our public data hub, on a
public mailing list (Google Groups) and on StackOverflow (programming Q&A
site). Quick and diverse help is immediately available for free from those
routes.

The platform's special features are fully documented. Everything else uses
standard Unix and open source tools, which are all documented by their upstream
projects, which have their own help and support groups.
-->

<!-- Service Levels inc. compensation -->

</div>
