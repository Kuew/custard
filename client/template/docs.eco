<nav class="well">
  <ul class="nav nav-list">
    <li class="nav-header">Datasets</li>
    <li><a data-nonpushstate href="#datasets-intro">Introduction</a></li>
    <li><a data-nonpushstate href="#datasets-basics">Dataset basics</a></li>
    <li><a data-nonpushstate href="#datasets-anatomy">Anatomy of a box</a></li>
    <li><a data-nonpushstate href="#datasets-endpoints">Box API endpoints</a></li>
    <li><a data-nonpushstate href="#datasets-services">Unix services</a></li>
    <li class="nav-header">Views</li>
    <li><a data-nonpushstate href="#">Home</a></li>
    <li><a data-nonpushstate href="#">Library</a></li>
    <li class="nav-header">Tools</li>
    <li><a data-nonpushstate href="#">Home</a></li>
    <li><a data-nonpushstate href="#">Library</a></li>
  </ul>
</nav>

<h2 id="datasets-intro">ScraperWiki is about datasets</h2>

<p>The simplest dataset is just a SQLite database, with a bit of code to create it and possibly update it. On ScraperWiki, these files live inside what we call a <b>box</b> &ndash; essentially a Unix shell account on the web. Boxes are sandboxed from each other, and provided with a set of API endpoints for communicating with the outside world.</p>

<hr />

<h2 id="datasets-basics">Dataset basics</h2>

<p>You can <b>create a new dataset</b> by visiting the <a href="/tools">My Tools</a> page, and clicking on the <b>"Code a dataset!"</b> tool.
This will show you how to SSH in.</p>

<p>For an existing dataset, you can use the <b>"View source"</b> tool, on your dataset&rsquo;s overview page, to see the same SSH instructions.</p>

<p class="well well-small"><span class="label label-info">Top tip!</span> If you&rsquo;ve never used SSH before, <a href="http://en.wikipedia.org/wiki/Ssh-keygen">this page about generating SSH keys</a> will come in handy.</p>

<hr />

<h2 id="datasets-anatomy">Anatomy of a box</h2>

<p>A box is a Unix user account on ScraperWiki's server cluster. The Unix user account has the same name as your box (eg: <code>by227hi</code>) and exists inside a <a href="http://en.wikipedia.org/wiki/Chroot">Chroot jail</a> for security and privacy. Your home directory is always <code>/home/</code>. </p>

<p>Because boxes are just Unix user accounts, all your favourite Unix tools like <code>scp</code>, <code>git</code>, and <code>cron</code> work right out of the box.  You have a permanent POSIX filesystem (it uses <a href="http://www.gluster.org/">GlusterFS</a>), limited to 500 MB storage across all the boxes you own. </p>

<p>Box settings, such as the <code>publish_token</code> and default database location, are stored in a JSON file at <code>~/scraperwiki.json</code>. Eg:</p>

<pre>{
  "database": "database.sqlite",
  "publish_token": "t5odv7of5l"
}</pre>

<p>Your box&rsquo;s <code>publish_token</code> is used to secure its public-facing, read-only endpoints (<code>/http</code> and <code>/sqlite</code>). It should be alphanumeric and 10-30 characters long. Or, if you&rsquo;d prefer to make your box&rsquo;s contents public, you can set it to an empty string or remove its entry entirely from <code>~/scraperwiki.json</code>.</p>

<hr />

<h2 id="datasets-endpoints">Box API endpoints</h2>

<h3>HTTP file endpoint</h3>

<p>Files placed in the <code>~/http/</code> directory of your box will be served statically via the box&rsquo;s HTTP endpoint. So, a file at <code>~/http/index.html</code> will be accessible at <code>https://box.scraperwiki.com/<em class="muted">&lt;box_name&gt;</em>/<em class="muted">&lt;publish_token&gt;</em>/http/index.html</code></p>

<p>The HTTP file endpoint is a great way to serve static web pages, client-side javascript apps, and downloadable files &ndash; especially when you&rsquo;re writing views, or setup screens for more complex datasets.</p>

<h3>SQL data endpoint</h3>

<p>If a SQLite file is named in the <code>database</code> key of <code>~/scraperwiki.json</code>, you can query it using the read-only SQL endpoint like so: <code>https://box.scraperwiki.com/<em class="muted">&lt;box_name&gt;</em>/<em class="muted">&lt;publish_token&gt;</em>/sqlite?q=select+*+from+sqlite_master</code>.</p>

<p>The SQL endpoint accepts only <code>HTTP GET</code> requests with the following parameters:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>GET parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>q</code></td>
    <td>The SQL query to execute. Multiple queries separated by a <code>;</code> are not allowed.</td>
  </tr>
  <tr>
    <td><code>callback</code></td>
    <td><span class="muted">[optional]</span> A callback function with which to wrap the JSON response, for JSONP output.</td>
  </tr>
</table>

<p>The SQL endpoint returns a JSON list of objects; one object for each row in the result set.</p>

<p>Example usage, with jQuery:</p>

<pre>$.ajax({
  url: 'https://box.scraperwiki.com/example/t5odv7of5l/sqlite',
  dataType: 'json',
  data: {
    'q': 'select * from sqlite_master'
  }
}).done(function(results){
  console.log(results)
})</pre>

<h3>Exec endpoint</h3>

<p>You can execute commands remotely, without SSHing in, by using your box&rsquo;s <code>/exec</code> endpoint. The Exec endpoint accepts <code>HTTP POST</code> requests with two required body parameters:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>POST parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>cmd</code></td>
    <td>The Unix command to execute inside the box. Multiple commands separated by a <code>;</code> <em>are</em> allowed. Commands are run from <code>/</code>.</td>
  </tr>
  <tr>
    <td><code>apikey</code></td>
    <td>The API Key of the box owner. (Hint: yours is <code><%= @user.apiKey %></code>)</td>
  </tr>
</table>

<p class="well well-small"><span class="label label-important">Watch out!</span> The Exec endpoint allows potentially destructive access to your box. Never share your API Key with anyone.</p>

<p>Because the Exec endpoint is secured using your <code>apikey</code>, there is no need to provide a <code>publish_token</code> in the URL. Eg:</p>

<pre>$.ajax({
  url: 'https://box.scraperwiki.com/example/exec', // note: no publish_token
  type: 'POST',
  data: {
    'cmd': 'echo "hello world" > hello.txt; ls -hitlar',
    'apikey': '<%= @user.apiKey %>'
  }
}).done(function(text){
  console.log(text)
})</pre>

<p>Unlike the other box endpoints, the Exec endpoint returns plain text, rather than JSON.</p>

<h3>File upload endpoint</h3>

<p>Boxes come with a file upload endpoint, allowing you to write datasets or views that accept a user&rsquo;s files as input. The file upload endpoint accepts <code>HTTP POST</code> requests, and like the Exec endpoint, requires your <code>apikey</code> as a body parameter:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>POST parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>file</code></td>
    <td>The file you wish to upload.</td>
  </tr>
  <tr>
    <td><code>apikey</code></td>
    <td>The API Key of the box owner. (Hint: yours is <code><%= @user.apiKey %></code>)</td>
  </tr>
  <tr>
    <td><code>next</code></td>
    <td>The URL to which users will be redirected once the files have been uploaded</td>
  </tr>
</table>

<p>You will often use the file upload endpoint as the <code>action</code> attribute of a web form, like so:</p>

<pre>&lt;!-- in /http/index.html --&gt;
&lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"&gt;&lt;/script&gt;
&lt;script src="http://x.scraperwiki.com/js/custard.js"&gt;&lt;/script&gt;
&lt;form id="up" action="../../file/" method="POST" enctype="multipart/form-data"&gt;
  &lt;input type="file" name="file" size="80" id="file"&gt;
  &lt;input type="hidden" name="apikey" id="apikey"&gt;
  &lt;input type="hidden" name="next" id="next"&gt;
  &lt;input type="submit" value="Upload now!"&gt;
&lt;/form&gt;
&lt;script&gt;
  settings = readSettings()
  $('#next').val(window.location.pathname + 'done.html' + window.location.hash)
  $('#apikey').val(settings.source.apikey)
&lt;/script&gt;</pre>

<p>The uploaded file will be put in the <code>/home/incoming/</code> directory.</p>

<h2 id="datasets-services">Unix services</h2>

<h3>Third party libraries</h3>

<p>We've installed many standard Unix tools.
</p>

<ul>
    <li>Languages such as Python, R, Ruby, PHP, Node, Java and Clojure.</li>
    <li>Scraping libraries such as Mechanize, Hpricot, Highrise, Zombie.</li>
    <li>Data using libraries such as Zombie, NLTK, iCalendar.</li>
    <li>Version control software such as git, Subversion and Mercurial.</li>
    <li>Useful tools like GNU Screen, strace and curl.</li>
    <li>Editors like vim and Emacs.</li>
</ul>

<h3>Cron</h3>

<p>You can create a standard cron job using the <code>crontab</code> command.
Unlike other facilities, this runs separately on each physical server in
ScraperWiki's cluster. Currently you only have access to one server &ndash; this will
change as we add more.
</p>

<h3>SSH</h3>

<p>This is an ordinary SSH server, you can use <code>scp</code>,
<code>sftp</code>, <code>git</code> over SSH and so on. Your keys are stored
separately for each box in <code>/home/.ssh/</code>, so you can add and
remove people.

<!-- <h3>SMTP</h3>

-->





<!-- Things to document:

scraperwiki.tool.getURL()

-->

