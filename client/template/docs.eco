<nav class="well">
  <ul class="nav nav-list">
    <li><a data-nonpushstate href="#scraperwiki-intro">Introduction</a></li>
    <li class="nav-header">Datasets</li>
    <li><a data-nonpushstate href="#datasets-basics">Dataset basics</a></li>
    <li><a data-nonpushstate href="#datasets-anatomy">Anatomy of a box</a></li>
    <li><a data-nonpushstate href="#datasets-endpoints">Box API endpoints</a></li>
    <li><a data-nonpushstate href="#datasets-services">Unix services</a></li>
    <li class="nav-header">Views</li>
    <li><a data-nonpushstate href="#views-basics">View basics</a></li>
    <li><a data-nonpushstate href="#views-styling">Styling your view</a></li>
    <li><a data-nonpushstate href="#views-helper">Helper library</a></li>
    <li class="nav-header">Tools</li>
    <li><a data-nonpushstate href="#tools-structure">Structure of a tool</a></li>
    <li><a data-nonpushstate href="#tools-process">Development process</a></li>
    <li><a data-nonpushstate href="#tools-submitting">Submitting your tool</a></li>
  </ul>
</nav>

<h2 id="scraperwiki-intro">ScraperWiki is about datasets</h2>

<p>The simplest dataset is just a SQLite database, with a bit of code to create it and possibly update it. </p>

<p>On ScraperWiki, these files live inside what we call a <b>box</b> &ndash; essentially a Unix shell account on the web. Boxes are sandboxed from each other, and provided with a set of API endpoints for communicating with the outside world.</p>

<p>Datasets can be analysed, visualised and exported using <b>views</b>. Each view is a separate box, and communicates with the dataset by making SQL queries and accessing files in the dataset over HTTP.</p>

<p>Datasets and views can be packaged up for you or other people to reuse and customise as <b>tools</b>. Tools are just git repositories, which are automatically checked out into a blank box when they are used to make a dataset or a view.</p>

<hr />

<h2 id="datasets-basics">Dataset basics</h2>

<p>You can <b>create a new dataset</b> by visiting the <a href="/tools">My Tools</a> page, and clicking on the <b>"Code a dataset!"</b> tool.
This will show you how to SSH in.</p>

<p>For an existing dataset, you can use the <b>"View source"</b> tool, on your dataset&rsquo;s overview page, to see the same SSH instructions.</p>

<p class="well well-small"><span class="label label-info">Top tip!</span> If you&rsquo;ve never used SSH before, <a href="http://en.wikipedia.org/wiki/Ssh-keygen">this page about generating SSH keys</a> will come in handy.</p>

<hr />

<h2 id="datasets-anatomy">Anatomy of a box</h2>

<p>A box is a Unix user account on ScraperWiki's server cluster. The Unix user account has the same name as your box (eg: <code>by227hi</code>) and exists inside a <a href="http://en.wikipedia.org/wiki/Chroot">Chroot jail</a> for security and privacy. Your home directory is always <code>/home/</code>. </p>

<p>Because boxes are just Unix user accounts, all your favourite Unix tools like <code>scp</code>, <code>git</code>, and <code>cron</code> work right out of the box.  You have a permanent POSIX filesystem (it uses <a href="http://www.gluster.org/">GlusterFS</a>), limited to 500 MB storage across all the boxes you own. </p>

<p>Box settings, such as the <code>publish_token</code> and default database location, are stored in a JSON file at <code>~/scraperwiki.json</code>. Eg:</p>

<pre class="prettyprint linenums">{
  "database": "database.sqlite",
  "publish_token": "t5odv7of5l"
}</pre>

<p>Your box&rsquo;s <code>publish_token</code> is used to secure its public-facing, read-only endpoints (<code>/http</code> and <code>/sqlite</code>). It should be alphanumeric and 10-30 characters long. Or, if you&rsquo;d prefer to make your box&rsquo;s contents public, you can set it to an empty string or remove its entry entirely from <code>~/scraperwiki.json</code>.</p>

<hr />

<h2 id="datasets-endpoints">Box API endpoints</h2>

<h3 id="datasets-endpoints-http">HTTP file endpoint</h3>

<p>Files placed in the <code>~/http/</code> directory of your box will be served statically via the box&rsquo;s HTTP endpoint. So, a file at <code>~/http/index.html</code> will be accessible at <code>https://box.scraperwiki.com/<em class="muted">&lt;box_name&gt;</em>/<em class="muted">&lt;publish_token&gt;</em>/http/index.html</code></p>

<p>The HTTP file endpoint is a great way to serve static web pages, client-side javascript apps, and downloadable files &ndash; especially when you&rsquo;re writing views, or setup screens for more complex datasets.</p>

<h3 id="datasets-endpoints-sql">SQL data endpoint</h3>

<p>If a SQLite file is named in the <code>database</code> key of <code>~/scraperwiki.json</code>, you can query it using the read-only SQL endpoint like so: <code>https://box.scraperwiki.com/<em class="muted">&lt;box_name&gt;</em>/<em class="muted">&lt;publish_token&gt;</em>/sqlite?q=select+*+from+sqlite_master</code>.</p>

<p>The SQL endpoint accepts only <code>HTTP GET</code> requests with the following parameters:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>GET&nbsp;parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>q</code></td>
    <td>The SQL query to execute. Multiple queries separated by a <code>;</code> are not allowed.</td>
  </tr>
  <tr>
    <td><code>callback</code></td>
    <td><span class="muted">[optional]</span> A callback function with which to wrap the JSON response, for JSONP output.</td>
  </tr>
</table>

<p>The SQL endpoint returns a JSON list of objects; one object for each row in the result set.</p>

<p>Example usage, with jQuery:</p>

<pre class="prettyprint linenums">$.ajax({
  url: 'https://box.scraperwiki.com/example/t5odv7of5l/sqlite',
  dataType: 'json',
  data: {
    'q': 'select * from sqlite_master'
  }
}).done(function(results){
  console.log(results)
})</pre>

<h3 id="datasets-endpoints-exec">Exec endpoint</h3>

<p>You can execute commands remotely, without SSHing in, by using your box&rsquo;s <code>/exec</code> endpoint. The Exec endpoint accepts <code>HTTP POST</code> requests with two required body parameters:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>POST&nbsp;parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>cmd</code></td>
    <td>The Unix command to execute inside the box. Multiple commands separated by a <code>;</code> <em>are</em> allowed. Commands are run from <code>/</code>.</td>
  </tr>
  <tr>
    <td><code>apikey</code></td>
    <td>The API Key of the box owner. (Hint: yours is <code><%= @user.apiKey %></code>)</td>
  </tr>
</table>

<p class="well well-small"><span class="label label-important">Watch out!</span> The Exec endpoint allows potentially destructive access to your box. Never share your API Key with anyone.</p>

<p>Because the Exec endpoint is secured using your <code>apikey</code>, there is no need to provide a <code>publish_token</code> in the URL. Eg:</p>

<pre class="prettyprint linenums">$.ajax({
  url: 'https://box.scraperwiki.com/example/exec', // note: no publish_token
  type: 'POST',
  data: {
    'cmd': 'echo "hello world" > hello.txt; ls -hitlar',
    'apikey': '<%= @user.apiKey %>'
  }
}).done(function(text){
  console.log(text)
})</pre>

<p>Unlike the other box endpoints, the Exec endpoint returns plain text, rather than JSON.</p>

<h3 id="datasets-endpoint-file">File upload endpoint</h3>

<p>Boxes come with a file upload endpoint, allowing you to write datasets or views that accept a user&rsquo;s files as input. The file upload endpoint accepts <code>HTTP POST</code> requests, and like the Exec endpoint, requires your <code>apikey</code> as a body parameter:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>POST&nbsp;parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>file</code></td>
    <td>The file you wish to upload.</td>
  </tr>
  <tr>
    <td><code>apikey</code></td>
    <td>The API Key of the box owner. (Hint: yours is <code><%= @user.apiKey %></code>)</td>
  </tr>
  <tr>
    <td><code>next</code></td>
    <td>The URL to which users will be redirected once the files have been uploaded</td>
  </tr>
</table>

<p>You will often use the file upload endpoint as the <code>action</code> attribute of a web form, like so:</p>

<pre class="prettyprint linenums">&lt;!-- in /http/index.html --&gt;
&lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"&gt;&lt;/script&gt;
&lt;script src="https://x.scraperwiki.com/js/scraperwiki.js"&gt;&lt;/script&gt;
&lt;form id="up" action="../../file/" method="POST" enctype="multipart/form-data"&gt;
  &lt;input type="file" name="file" size="80" id="file"&gt;
  &lt;input type="hidden" name="apikey" id="apikey"&gt;
  &lt;input type="hidden" name="next" id="next"&gt;
  &lt;input type="submit" value="Upload now!"&gt;
&lt;/form&gt;
&lt;script&gt;
  settings = scraperwiki.readSettings()
  $('#next').val(window.location.pathname + 'done.html' + window.location.hash)
  $('#apikey').val(settings.source.apikey)
&lt;/script&gt;</pre>

<p>The uploaded file will be put in the <code>/home/incoming/</code> directory.</p>

<h3 id="datasets-endpoints-status">Dataset status API</h3>

<p>The ScraperWiki website can display the status of your datasets (for example, when they last ran, and whether they encountered errors). You can register the status of your dataset by making a <code>HTTP POST</code> request, <em class="text-error">from within the box</em>, to <code>https://x.scraperwiki.com/api/status</code>.</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>POST&nbsp;parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>type</code></td>
    <td>Your dataset&rsquo;s status. Should either be <code>ok</code> or <code>error</code>.</td>
  </tr>
  <tr>
    <td><code>message</code></td>
    <td>A description, such as <code>Scraped 24 new tweets</code> or <code>Invalid password</code>.</td>
  </tr>
</table>

<p>This API requires no <code>publish_token</code> or <code>apikey</code> because it automatically detects the credentials of the box from which it&rsquo;s called. Magic!</p>

<p>The endpoint returns a JSON object with a single <code>success</code> key on success, or an <code>error</code> key on errors.</p>

<hr />

<h2 id="datasets-services">Unix services</h2>

<h3>Third party libraries</h3>

<p>We've installed many standard Unix tools.
</p>

<ul>
    <li>Languages such as Python, R, Ruby, PHP, Node, Java and Clojure.</li>
    <li>Scraping libraries such as Mechanize, Hpricot, Highrise, Zombie.</li>
    <li>Data using libraries such as Zombie, NLTK, iCalendar.</li>
    <li>Version control software such as git, Subversion and Mercurial.</li>
    <li>Useful tools like GNU Screen, strace and curl.</li>
    <li>Editors like vim and Emacs.</li>
</ul>

<h3>Cron</h3>

<p>You can create a standard cron job using the <code>crontab</code> command.
Unlike other facilities, this runs separately on each physical server in
ScraperWiki's cluster. Currently you only have access to one server &ndash; this will
change as we add more.
</p>

<h3>SSH</h3>

<p>This is an ordinary SSH server, you can use <code>scp</code>,
<code>sftp</code>, <code>git</code> over SSH and so on. Your keys are stored
separately for each box in <code>/home/.ssh/</code>, so you can add and
remove people.

<!-- <h3>SMTP</h3>

-->

<hr />

<h2 id="views-basics">View basics</h2>

<p>You can <b>create a new view</b> clicking on the <b>"Code a view!"</b> tool on a dataset overview page. This will show you how to SSH in.</p>

<p class="well well-small"><span class="label label-info">Top tip!</span> You can find out how to SSH into any dataset or view by looking for its seven character identifier in the URL, and using it as the user name in your SSH command.</p>

<p>A view is just HTML, starting at the file <code>http/index.html</code>. Visualisations are a combination of HTML, Javascript, and calls to run arbitary commands inside the box if you need them. The view loads inside an <code>iframe</code>.
</p>

<hr />

<h2 id="views-styling">Styling your view</h2>

<p>It's important that views share the same styling and fit in with the rest of ScraperWiki.
You'll want to include Bootstrap and our custom style sheet.
</p>

<pre class="prettyprint">&lt;link rel="stylesheet" href="http://x.scraperwiki.com/vendor/style/bootstrap.min.css"&gt; 
&lt;link rel="stylesheet" href="http://x.scraperwiki.com/vendor/style/metro.bootstrap.css"&gt; 
&lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"&gt;&lt;/script&gt; 
&lt;script src="http://x.scraperwiki.com/vendor/js/bootstrap.min.js"&gt;&lt;/script&gt;</pre>

<p>Write your HTML in a way that <a href="http://twitter.github.com/bootstrap/base-css.html">works
well with Boostrap</a>.
</p>

<hr />

<h2 id="views-helper">Helper library</h2>

<p>There's a client side Javascript library, which wraps a lot of the API
endpoints and functions needed by views. You'll want to include it in
your view's HTML <code>&lt;head&gt;</code>.</p>

<pre class="prettyprint">&lt;script src="https://x.scraperwiki.com/js/scraperwiki.js"&gt;&lt;/script&gt;</pre>

<h3>Read settings</h3>

<p>When somebody loads your view, ScraperWiki passes it a number of settings. The settings are
in a URL encoded JSON object after the # in the URL. You can use this helper function
to access them easier:

<pre class="prettyprint">
scraperwiki.readSettings()
</pre>

<p>This will return a JSON object where <code>target</code> contains information about the box of the dataset being visualised
and <code>source</code> contains information about the view's box.
</p>

<pre class="prettyprint linenums">{
    "source": {
        "apikey": "<%= @user.apiKey %>"
        "token": "t5odv7of5l"
        "url": "https://box.scraperwiki.com/fdtlza1/t5odv7of5l"
    }, 
    "target": {
        "token": "a1pax8jk32"
        "url": "https://box.scraperwiki.com/de3jikz/a1pax8jk32"
    }
}</pre>

<p>For example, this gets the target dataset's <a data-nonpushstate href="#datasets-endpoints-sql">SQL data endpoint</a>.</p>

<pre class="prettyprint">
scraperwiki.readSettings().target.url + "/sqlite"
</pre>

<h3>Box name</h3>

<p>The name (random letters and numbers) of the current box is in <code>scraperwiki.boxName</code>.

</p>

<h3>Redirecting</h3>

<p>Since you're in a secure iframe, you need to call a special function to 
redirect the browser to another location. For example, this redirects to
the dataset overview page:
</p>

<pre class="prettyprint linenums">var datasetUrl = "/dataset/" + scraperwiki.boxName
scraperwiki.tool.redirect(datasetUrl)</pre>

<p>It's also useful for redirecting to OAuth endpoints.</p>

<h3>Reading the URL</h3>

<p>Sometimes you will need the URL of the outer page, for example to
read query parameters or to get a URL for OAuth to redirect back to.
Getting the URL happens asynchronusly using <a href="http://easyxdm.net/wp/">XDM</a>,
so you need to pass in a callback function.
</p>

<pre class="prettyprint linenums">scraperwiki.tool.getURL(function(url) {
    console.log(url)
})</pre>


<hr />

<h2 id="tools-structure">Structure of a tool</h2>

<p>Tools are packaged datasets or views &ndash; files in a git repository which are built to run automatically when installed into a new box. By way of example, here are the contents of the <a href="https://github.com/scraperwiki/spreadsheet-upload-tool">Spreadsheet upload tool</a>:</p>

<pre class="prettyprint">code/
 └ extract.py
http/
 ├ done.html
 ├ index.html
 └ style.css
test/
 ├ simple.py
 └ tricky.py
README.md
scraperwiki.json</pre>

<p>When someone activates the Upload Spreadsheet tool, ScraperWiki creates a new box, checks the Git repository out into the box&rsquo;s <code>/home/tool/</code> directory, and shows the tool&rsquo;s <code>http/index.html</code> file to the user in an iframe. This <code>index.html</code> contains javascript that reads settings and generates a user interface for selecting a spreadsheet to upload. It works just like the <a href="views-basics">views</a> described above, and uses the <a href="#datasets-endpoints">box API endpoints</a> to select data and run server-side code.</p>

<p>The <code>scraperwiki.json</code> file contains the following settings describing the tool:</p>

<pre class="prettyprint linenums">{
  "displayName": "Upload a Spreadsheet",
  "gitUrl": "git://github.com/scraperwiki/spreadsheet-download-tool.git",
  "description": "Upload an Excel file or CSV"
}</pre>

<p>A <code>README.md</code> file is included in the Git repo so <a href="https://github.com/scraperwiki/spreadsheet-upload-tool">collaborators on Github</a> know what the tool does. This file is not read by ScraperWiki. You might want to put technical configuration instructions in here.</p>

<p>The <code>tool/</code> directory contains Python unit tests used during development. We suggest you write tests for your tools, expecially when listing them publically in the ScraperWiki Data Store.</p>

<hr />

<h2 id="tools-process">Development process</h2>

<!-- VCS workflows: git pushing, github checking out and stuff -->

<hr />

<h2 id="tools-submitting">Submitting your tool</h2>

<p>To sumbit a completed tool (or work in progress!) to ScraperWiki, you should call our Tools API endpoint:</p>



<!-- Javascript console adding a tool -->


<!-- Other things to document or fix and document:

Exec endpoint helper

Status updates in datasets

SMTP for other people

-->


